{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12565129,"sourceType":"datasetVersion","datasetId":7934720}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Vietnamese News Sentiment Analysis with XLM-RoBERTa**\n# This notebook fine-tunes XLM-RoBERTa for classifying sentiment of Vietnamese news summaries.\n* Dataset: 12007 samples","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers datasets evaluate accelerate scikit-learn pandas matplotlib seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:57:19.861056Z","iopub.execute_input":"2025-07-24T12:57:19.861866Z","iopub.status.idle":"2025-07-24T12:58:45.740986Z","shell.execute_reply.started":"2025-07-24T12:57:19.861838Z","shell.execute_reply":"2025-07-24T12:58:45.740193Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset, DatasetDict\nimport evaluate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport logging","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:58:45.742472Z","iopub.execute_input":"2025-07-24T12:58:45.742743Z","iopub.status.idle":"2025-07-24T12:59:27.452851Z","shell.execute_reply.started":"2025-07-24T12:58:45.742718Z","shell.execute_reply":"2025-07-24T12:59:27.452255Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 12:59:06.648050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753361947.014892      77 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753361947.124677      77 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Configuration\nclass Config:\n    MODEL_NAME = \"xlm-roberta-base\"\n    SEED = 42\n    BATCH_SIZE = 8  # Reduced to avoid GPU memory issues\n    GRADIENT_ACCUMULATION_STEPS = 2  # Adjusted for effective batch size of 16\n    LEARNING_RATE = 2e-5\n    NUM_EPOCHS = 10\n    MAX_LENGTH = 256\n    WEIGHT_DECAY = 0.01\n    OUTPUT_DIR = \"./xlm-roberta-sentiment-complete\"\n    LOGGING_STEPS = 10  # Increased frequency for better monitoring\n    SAVE_TOTAL_LIMIT = 2\n    SENTIMENT_MAP = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n    REVERSE_SENTIMENT_MAP = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n    EARLY_STOPPING_PATIENCE = 3\n    LR_SCHEDULER_TYPE = \"cosine\"\n    WARMUP_RATIO = 0.1\n    USE_CLASS_WEIGHTS = True\n    DATA_PATH = \"/kaggle/input/data-news-v2/data_news_v2.xlsx\"\n\nconfig = Config()\n\n# Create output directory\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:27.453709Z","iopub.execute_input":"2025-07-24T12:59:27.454333Z","iopub.status.idle":"2025-07-24T12:59:27.459823Z","shell.execute_reply.started":"2025-07-24T12:59:27.454310Z","shell.execute_reply":"2025-07-24T12:59:27.458925Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Set up logging with error handling\ntry:\n    logging.basicConfig(\n        filename=os.path.join(config.OUTPUT_DIR, 'training.log'),\n        level=logging.DEBUG,  # Increased verbosity\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n    logger = logging.getLogger(__name__)\n    logger.info(\"Logging initialized successfully\")\nexcept PermissionError:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info(\"Fallback to console logging due to permission error for file: %s\", os.path.join(config.OUTPUT_DIR, 'training.log'))\nexcept Exception as e:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info(\"Logging setup failed with error: %s. Fallback to console logging.\", str(e))\n\n# Set random seed\ntorch.manual_seed(config.SEED)\nnp.random.seed(config.SEED)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:27.461739Z","iopub.execute_input":"2025-07-24T12:59:27.462136Z","iopub.status.idle":"2025-07-24T12:59:27.548272Z","shell.execute_reply.started":"2025-07-24T12:59:27.462116Z","shell.execute_reply":"2025-07-24T12:59:27.547503Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Custom Trainer with class weights\nclass WeightedTrainer(Trainer):\n    def __init__(self, class_weights=None, **kwargs):\n        super().__init__(**kwargs)\n        self.class_weights = class_weights\n        \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        labels = labels.long()\n        \n        if self.class_weights is not None:\n            weights = torch.tensor(self.class_weights, device=logits.device, dtype=torch.float32)\n            loss_fct = torch.nn.CrossEntropyLoss(weight=weights)\n        else:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            \n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:27.548998Z","iopub.execute_input":"2025-07-24T12:59:27.549233Z","iopub.status.idle":"2025-07-24T12:59:27.554817Z","shell.execute_reply.started":"2025-07-24T12:59:27.549215Z","shell.execute_reply":"2025-07-24T12:59:27.554231Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load and explore dataset\ndef load_and_explore_data(file_path):\n    logger.info(\"Loading dataset...\")\n    df = pd.read_excel(file_path)\n    \n    df['label'] = df['sentiment'].map(config.SENTIMENT_MAP)\n    \n    with open(os.path.join(config.OUTPUT_DIR, 'dataset_info.txt'), 'w') as f:\n        f.write(f\"Total samples: {len(df)}\\n\")\n        f.write(\"\\nClass distribution:\\n\")\n        f.write(df['sentiment'].value_counts().to_string())\n    \n    plt.figure(figsize=(8, 5))\n    class_dist = df['sentiment'].value_counts()\n    sns.barplot(x=class_dist.index, y=class_dist.values)\n    plt.title('Class Distribution')\n    plt.ylabel('Count')\n    plt.savefig(os.path.join(config.OUTPUT_DIR, 'class_distribution.png'))\n    plt.close()\n    \n    df['text_length'] = df['summary'].apply(lambda x: len(x.split()))\n    \n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    sns.histplot(df['text_length'], bins=30)\n    plt.title('Text Length Distribution')\n    \n    plt.subplot(1, 2, 2)\n    sns.boxplot(x='sentiment', y='text_length', data=df)\n    plt.title('Text Length by Sentiment')\n    plt.savefig(os.path.join(config.OUTPUT_DIR, 'text_length_distribution.png'))\n    plt.close()\n    \n    return df\n\ndf = load_and_explore_data(config.DATA_PATH)\ndf = df.dropna()\n\nif config.USE_CLASS_WEIGHTS:\n    class_weights = compute_class_weight(\n        'balanced', \n        classes=np.unique(df['label']),\n        y=df['label']\n    )\n    config.CLASS_WEIGHTS = class_weights.tolist()\n    logger.info(f\"Class weights: {config.CLASS_WEIGHTS}\")\nelse:\n    config.CLASS_WEIGHTS = None\n\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:27.555723Z","iopub.execute_input":"2025-07-24T12:59:27.556015Z","iopub.status.idle":"2025-07-24T12:59:36.657212Z","shell.execute_reply.started":"2025-07-24T12:59:27.555991Z","shell.execute_reply":"2025-07-24T12:59:36.656508Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471c5daf6fb3483c801a1abed4f17872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85bed831cc734ca1b02c356a42adeb1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64111002c9141b08231b0eb7d15565c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b8be8ccd70458d9c6f22a1059d6daf"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Analyze token lengths\ndef analyze_token_lengths(texts, tokenizer, max_length):\n    lengths = []\n    for text in texts:\n        tokens = tokenizer(text, truncation=True, max_length=max_length)[\"input_ids\"]\n        lengths.append(len(tokens))\n    return lengths\n\ntoken_lengths = analyze_token_lengths(df['summary'], tokenizer, config.MAX_LENGTH)\n\nplt.figure(figsize=(10, 5))\nsns.histplot(token_lengths, bins=30)\nplt.title('Token Length Distribution')\nplt.axvline(x=config.MAX_LENGTH, color='r', linestyle='--', label='Max Length')\nplt.legend()\nplt.savefig(os.path.join(config.OUTPUT_DIR, 'token_length_distribution.png'))\nplt.close()\n\nlogger.info(f\"Percentage of texts within max length: {sum(np.array(token_lengths) <= config.MAX_LENGTH) / len(token_lengths):.2%}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:36.658043Z","iopub.execute_input":"2025-07-24T12:59:36.658614Z","iopub.status.idle":"2025-07-24T12:59:41.328085Z","shell.execute_reply.started":"2025-07-24T12:59:36.658590Z","shell.execute_reply":"2025-07-24T12:59:41.327234Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Preprocess function\ndef preprocess_function(examples):\n    return tokenizer(\n        examples[\"summary\"],\n        truncation=True,\n        max_length=config.MAX_LENGTH,\n        padding=\"max_length\"\n    )\n\n# Split data\ntrain_df, temp_df = train_test_split(\n    df,\n    test_size=0.2,\n    random_state=config.SEED,\n    stratify=df['label']\n)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    random_state=config.SEED,\n    stratify=temp_df['label']\n)\n\ntrain_dataset = Dataset.from_pandas(train_df[['summary', 'sentiment', 'label']])\nval_dataset = Dataset.from_pandas(val_df[['summary', 'sentiment', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['summary', 'sentiment', 'label']])\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\ntokenized_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=[\"summary\", \"sentiment\"]\n)\n\nwith open(os.path.join(config.OUTPUT_DIR, 'data_splits.txt'), 'w') as f:\n    f.write(f\"Train samples: {len(train_df)}\\n\")\n    f.write(f\"Validation samples: {len(val_df)}\\n\")\n    f.write(f\"Test samples: {len(test_df)}\\n\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    config.MODEL_NAME,\n    num_labels=3\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:41.329096Z","iopub.execute_input":"2025-07-24T12:59:41.329318Z","iopub.status.idle":"2025-07-24T12:59:50.156408Z","shell.execute_reply.started":"2025-07-24T12:59:41.329298Z","shell.execute_reply":"2025-07-24T12:59:50.155804Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3968a184955d4f35b279f29d5228c958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/935 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3f387f02cd4b598a0b6a38e390901c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/935 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edac6b3e5a75473e827beab800839861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16579b0cfb249a78ccb29f9b82b0707"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    f1_micro = f1_score(labels, predictions, average='micro')\n    f1_macro = f1_score(labels, predictions, average='macro')\n    f1_weighted = f1_score(labels, predictions, average='weighted')\n    \n    report = classification_report(\n        labels,\n        predictions,\n        target_names=['Negative', 'Neutral', 'Positive'],\n        output_dict=True\n    )\n    \n    metrics = {\n        'accuracy': accuracy,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'negative_precision': report['Negative']['precision'],\n        'negative_recall': report['Negative']['recall'],\n        'negative_f1': report['Negative']['f1-score'],\n        'neutral_precision': report['Neutral']['precision'],\n        'neutral_recall': report['Neutral']['recall'],\n        'neutral_f1': report['Neutral']['f1-score'],\n        'positive_precision': report['Positive']['precision'],\n        'positive_recall': report['Positive']['recall'],\n        'positive_f1': report['Positive']['f1-score']\n    }\n    \n    logger.info(f\"Evaluation metrics: {metrics}\")\n    \n    return metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:50.157220Z","iopub.execute_input":"2025-07-24T12:59:50.157502Z","iopub.status.idle":"2025-07-24T12:59:50.165560Z","shell.execute_reply.started":"2025-07-24T12:59:50.157471Z","shell.execute_reply":"2025-07-24T12:59:50.165019Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    run_name=f\"xlm-roberta-sentiment-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=config.LOGGING_STEPS,\n    save_steps=100,\n    save_total_limit=config.SAVE_TOTAL_LIMIT,\n    learning_rate=config.LEARNING_RATE,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE,\n    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n    num_train_epochs=config.NUM_EPOCHS,\n    weight_decay=config.WEIGHT_DECAY,\n    lr_scheduler_type=config.LR_SCHEDULER_TYPE,\n    warmup_ratio=config.WARMUP_RATIO,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1_macro\",\n    greater_is_better=True,\n    fp16=True,\n    logging_dir=\"./logs\",\n    seed=config.SEED,\n    report_to=\"none\",  # Disable wandb logging\n    log_level=\"debug\"  # Increase logging verbosity\n)\n\n# Initialize Trainer\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    class_weights=config.CLASS_WEIGHTS,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=config.EARLY_STOPPING_PATIENCE)]\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:50.167773Z","iopub.execute_input":"2025-07-24T12:59:50.168015Z","iopub.status.idle":"2025-07-24T12:59:50.851484Z","shell.execute_reply.started":"2025-07-24T12:59:50.167993Z","shell.execute_reply":"2025-07-24T12:59:50.850908Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_77/1477917787.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(**kwargs)\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Check GPU memory before training\nlogger.info(f\"GPU available: {torch.cuda.is_available()}\")\nlogger.info(f\"GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated, {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated, {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")\n\n# Start training with error handling\ntry:\n    logger.info(\"Starting training...\")\n    print(\"Starting training...\")\n    train_result = trainer.train()\nexcept Exception as e:\n    logger.error(f\"Training failed with error: {str(e)}\")\n    print(f\"Training failed with error: {str(e)}\")\n    raise e\n\n# Save training metrics\nmetrics = train_result.metrics\ntrainer.save_metrics(\"train\", metrics)\nlogger.info(f\"Training metrics: {metrics}\")\n\n# Save the final model\ntrainer.save_model(config.OUTPUT_DIR)\ntokenizer.save_pretrained(config.OUTPUT_DIR)\nlogger.info(f\"Model saved to {config.OUTPUT_DIR}\")\n\n# Save training arguments\ntrainer.save_state()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:59:50.852170Z","iopub.execute_input":"2025-07-24T12:59:50.852375Z","iopub.status.idle":"2025-07-24T13:27:13.175151Z","shell.execute_reply.started":"2025-07-24T12:59:50.852358Z","shell.execute_reply":"2025-07-24T13:27:13.174503Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 16\nThe following columns in the Training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 7,477\n  Num Epochs = 10\n  Instantaneous batch size per device = 8\n  Training with DataParallel so batch size has been adjusted to: 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 2\n  Total optimization steps = 2,340\n  Number of trainable parameters = 278,045,955\n","output_type":"stream"},{"name":"stdout","text":"GPU available: True\nGPU memory: 1.04 GB allocated, 1.09 GB reserved\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1100' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1100/2340 27:15 < 30:46, 0.67 it/s, Epoch 4/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>F1 Weighted</th>\n      <th>Negative Precision</th>\n      <th>Negative Recall</th>\n      <th>Negative F1</th>\n      <th>Neutral Precision</th>\n      <th>Neutral Recall</th>\n      <th>Neutral F1</th>\n      <th>Positive Precision</th>\n      <th>Positive Recall</th>\n      <th>Positive F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.012700</td>\n      <td>0.919944</td>\n      <td>0.688770</td>\n      <td>0.688770</td>\n      <td>0.625887</td>\n      <td>0.662897</td>\n      <td>0.651558</td>\n      <td>0.867925</td>\n      <td>0.744337</td>\n      <td>0.488189</td>\n      <td>0.256198</td>\n      <td>0.336043</td>\n      <td>0.773626</td>\n      <td>0.822430</td>\n      <td>0.797282</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.638300</td>\n      <td>0.622792</td>\n      <td>0.747594</td>\n      <td>0.747594</td>\n      <td>0.723928</td>\n      <td>0.745941</td>\n      <td>0.785455</td>\n      <td>0.815094</td>\n      <td>0.800000</td>\n      <td>0.560870</td>\n      <td>0.533058</td>\n      <td>0.546610</td>\n      <td>0.823256</td>\n      <td>0.827103</td>\n      <td>0.825175</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.630500</td>\n      <td>0.600954</td>\n      <td>0.758289</td>\n      <td>0.758289</td>\n      <td>0.743386</td>\n      <td>0.761799</td>\n      <td>0.822394</td>\n      <td>0.803774</td>\n      <td>0.812977</td>\n      <td>0.559259</td>\n      <td>0.623967</td>\n      <td>0.589844</td>\n      <td>0.849754</td>\n      <td>0.806075</td>\n      <td>0.827338</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.603100</td>\n      <td>0.584761</td>\n      <td>0.772193</td>\n      <td>0.772193</td>\n      <td>0.734532</td>\n      <td>0.759844</td>\n      <td>0.752351</td>\n      <td>0.905660</td>\n      <td>0.821918</td>\n      <td>0.654545</td>\n      <td>0.446281</td>\n      <td>0.530713</td>\n      <td>0.829268</td>\n      <td>0.873832</td>\n      <td>0.850967</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.459900</td>\n      <td>0.632859</td>\n      <td>0.771123</td>\n      <td>0.771123</td>\n      <td>0.738145</td>\n      <td>0.762164</td>\n      <td>0.750000</td>\n      <td>0.916981</td>\n      <td>0.825127</td>\n      <td>0.628415</td>\n      <td>0.475207</td>\n      <td>0.541176</td>\n      <td>0.848131</td>\n      <td>0.848131</td>\n      <td>0.848131</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.513900</td>\n      <td>0.617253</td>\n      <td>0.768984</td>\n      <td>0.768984</td>\n      <td>0.745819</td>\n      <td>0.766383</td>\n      <td>0.905405</td>\n      <td>0.758491</td>\n      <td>0.825462</td>\n      <td>0.592920</td>\n      <td>0.553719</td>\n      <td>0.572650</td>\n      <td>0.788501</td>\n      <td>0.897196</td>\n      <td>0.839344</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.503400</td>\n      <td>0.604859</td>\n      <td>0.780749</td>\n      <td>0.780749</td>\n      <td>0.759165</td>\n      <td>0.777821</td>\n      <td>0.882591</td>\n      <td>0.822642</td>\n      <td>0.851563</td>\n      <td>0.613636</td>\n      <td>0.557851</td>\n      <td>0.584416</td>\n      <td>0.805556</td>\n      <td>0.880841</td>\n      <td>0.841518</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.376600</td>\n      <td>0.614865</td>\n      <td>0.788235</td>\n      <td>0.788235</td>\n      <td>0.773507</td>\n      <td>0.788848</td>\n      <td>0.840000</td>\n      <td>0.871698</td>\n      <td>0.855556</td>\n      <td>0.616935</td>\n      <td>0.632231</td>\n      <td>0.624490</td>\n      <td>0.856796</td>\n      <td>0.824766</td>\n      <td>0.840476</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.399400</td>\n      <td>0.663538</td>\n      <td>0.765775</td>\n      <td>0.765775</td>\n      <td>0.743936</td>\n      <td>0.763394</td>\n      <td>0.785235</td>\n      <td>0.883019</td>\n      <td>0.831261</td>\n      <td>0.591111</td>\n      <td>0.549587</td>\n      <td>0.569593</td>\n      <td>0.847087</td>\n      <td>0.815421</td>\n      <td>0.830952</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.238600</td>\n      <td>0.745242</td>\n      <td>0.777540</td>\n      <td>0.777540</td>\n      <td>0.766634</td>\n      <td>0.781195</td>\n      <td>0.806228</td>\n      <td>0.879245</td>\n      <td>0.841155</td>\n      <td>0.588448</td>\n      <td>0.673554</td>\n      <td>0.628131</td>\n      <td>0.897019</td>\n      <td>0.773364</td>\n      <td>0.830615</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.296600</td>\n      <td>0.675462</td>\n      <td>0.779679</td>\n      <td>0.779679</td>\n      <td>0.766362</td>\n      <td>0.782595</td>\n      <td>0.840909</td>\n      <td>0.837736</td>\n      <td>0.839319</td>\n      <td>0.591760</td>\n      <td>0.652893</td>\n      <td>0.620825</td>\n      <td>0.863861</td>\n      <td>0.815421</td>\n      <td>0.838942</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-100\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-100/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-100/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-100/special_tokens_map.json\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-200\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-200/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-200/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-200/special_tokens_map.json\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-300\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-300/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-300/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-300/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-100] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-400\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-400/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-400/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-400/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-200] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-500\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-500/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-500/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-500/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-400] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-600\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-600/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-600/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-600/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-600/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-300] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-700\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-700/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-700/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-700/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-700/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-500] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-800\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-800/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-800/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-800/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-800/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-600] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-900\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-900/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-900/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-900/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-900/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-700] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-1000\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-1000/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-1000/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-900] due to args.save_total_limit\nThe following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\nSaving model checkpoint to ./xlm-roberta-sentiment-complete/checkpoint-1100\nConfiguration saved in ./xlm-roberta-sentiment-complete/checkpoint-1100/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/checkpoint-1100/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/checkpoint-1100/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/checkpoint-1100/special_tokens_map.json\nDeleting older checkpoint [xlm-roberta-sentiment-complete/checkpoint-1000] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./xlm-roberta-sentiment-complete/checkpoint-800 (score: 0.7735071806500379).\nSaving model checkpoint to ./xlm-roberta-sentiment-complete\nConfiguration saved in ./xlm-roberta-sentiment-complete/config.json\nModel weights saved in ./xlm-roberta-sentiment-complete/model.safetensors\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/special_tokens_map.json\ntokenizer config file saved in ./xlm-roberta-sentiment-complete/tokenizer_config.json\nSpecial tokens file saved in ./xlm-roberta-sentiment-complete/special_tokens_map.json\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Evaluate on test set\nlogger.info(\"Evaluating on test set...\")\nprint(\"Evaluating on test set...\")\ntest_results = trainer.evaluate(\n    tokenized_datasets[\"test\"],\n    metric_key_prefix=\"test\"\n)\n\n# Save evaluation results\nwith open(os.path.join(config.OUTPUT_DIR, 'test_results.txt'), 'w') as f:\n    for key, value in test_results.items():\n        f.write(f\"{key}: {value}\\n\")\n\nlogger.info(\"\\n=== Test Results ===\")\nprint(\"\\n=== Test Results ===\")\nfor key, value in test_results.items():\n    if key.startswith(\"test_\"):\n        logger.info(f\"{key[5:]}: {value}\")\n        print(f\"{key[5:]}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:27:13.176004Z","iopub.execute_input":"2025-07-24T13:27:13.176244Z","iopub.status.idle":"2025-07-24T13:27:26.487205Z","shell.execute_reply.started":"2025-07-24T13:27:13.176226Z","shell.execute_reply":"2025-07-24T13:27:26.486467Z"}},"outputs":[{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 935\n  Batch size = 16\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [59/59 00:13]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"early stopping required metric_for_best_model, but did not find eval_f1_macro so early stopping is disabled\n","output_type":"stream"},{"name":"stdout","text":"\n=== Test Results ===\nloss: 0.6309393048286438\naccuracy: 0.758288770053476\nf1_micro: 0.758288770053476\nf1_macro: 0.7444634862297465\nf1_weighted: 0.7619015516906373\nnegative_precision: 0.784452296819788\nnegative_recall: 0.8345864661654135\nnegative_f1: 0.8087431693989071\nneutral_precision: 0.5682656826568265\nneutral_recall: 0.6363636363636364\nneutral_f1: 0.6003898635477583\npositive_precision: 0.8740157480314961\npositive_recall: 0.7798594847775175\npositive_f1: 0.8242574257425742\nruntime: 13.2986\nsamples_per_second: 70.308\nsteps_per_second: 4.437\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Sample predictions function\ndef predict_sentiment(text):\n    inputs = tokenizer(\n        text,\n        max_length=config.MAX_LENGTH,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    ).to(trainer.model.device)\n    \n    with torch.no_grad():\n        outputs = trainer.model(**inputs)\n    \n    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    pred_class = torch.argmax(probs).item()\n    \n    return {\n        \"sentiment\": config.REVERSE_SENTIMENT_MAP[pred_class],\n        \"confidence\": probs[0][pred_class].item(),\n        \"probabilities\": {\n            \"Negative\": probs[0][0].item(),\n            \"Neutral\": probs[0][1].item(),\n            \"Positive\": probs[0][2].item()\n        }\n    }\n\n# Test on some samples and save predictions\nsample_texts = df.sample(5, random_state=config.SEED)[\"summary\"].tolist()\nwith open(os.path.join(config.OUTPUT_DIR, 'sample_predictions.txt'), 'w') as f:\n    for i, text in enumerate(sample_texts):\n        result = predict_sentiment(text)\n        actual = df[df['summary'] == text]['sentiment'].values[0]\n        \n        f.write(f\"\\n=== Sample {i+1} ===\\n\")\n        f.write(f\"\\nText: {text}\\n\")\n        f.write(f\"\\nPredicted Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2f})\\n\")\n        f.write(f\"Probabilities: {result['probabilities']}\\n\")\n        f.write(f\"Actual Sentiment: {actual}\\n\")\n        \n        logger.info(f\"Sample {i+1} - Predicted: {result['sentiment']}, Actual: {actual}\")\n        print(f\"\\n=== Sample {i+1} ===\")\n        print(f\"\\nText: {text}\")\n        print(f\"\\nPredicted Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2f})\")\n        print(f\"Probabilities: {result['probabilities']}\")\n        print(f\"Actual Sentiment: {actual}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:27:26.488303Z","iopub.execute_input":"2025-07-24T13:27:26.488545Z","iopub.status.idle":"2025-07-24T13:27:26.835940Z","shell.execute_reply.started":"2025-07-24T13:27:26.488526Z","shell.execute_reply":"2025-07-24T13:27:26.835166Z"}},"outputs":[{"name":"stdout","text":"\n=== Sample 1 ===\n\nText: Chủ tịch Hội đồng thành viên EVN tiếp và làm việc với lãnh đạo KEPCO về ứng dụng công nghệ thông minh và phát triển nguồn điện hạt nhân. Các bên muốn tăng cường hợp tác, chia sẻ kinh nghiệm để phát triển dự án điện. EVN đề xuất thành lập tổ công tác chuyên môn. KEPCO đang quản lý 83GW và tham gia đầu tư dự án BOT tại Việt Nam. Nhiều đối tác nước ngoài muốn hợp tác với Việt Nam trong dự án điện hạt nhân Ninh Thuận, bao gồm Mỹ, Hàn Quốc, Nga, Nhật, Trung Quốc, Pháp.\n\nPredicted Sentiment: Positive (Confidence: 0.87)\nProbabilities: {'Negative': 0.00490963738411665, 'Neutral': 0.12209127843379974, 'Positive': 0.8729991912841797}\nActual Sentiment: Positive\n\n=== Sample 2 ===\n\nText: Từ ngày 1/7, Việt Nam chỉ còn 34 tỉnh, thành phố sau khi sắp xếp lại hành chính từ 63 đơn vị trước đây. Việc này nhằm tạo điều kiện thuận lợi hơn cho các địa phương thu hút vốn đầu tư trực tiếp nước ngoài (FDI). Các “thủ phủ” mới đã được hình thành và hi vọng sẽ thu hút được nhiều vốn FDI hơn trong thời gian tới.\n\nPredicted Sentiment: Positive (Confidence: 0.91)\nProbabilities: {'Negative': 0.00754185626283288, 'Neutral': 0.0784013643860817, 'Positive': 0.9140567779541016}\nActual Sentiment: Positive\n\n=== Sample 3 ===\n\nText: Trong bối cảnh cần tái cấu trúc doanh nghiệp nhà nước tại TPHCM, các chuyên gia nhấn mạnh về việc định vị lại phạm vi hoạt động và nâng cao quản trị. Việc xác định vai trò của doanh nghiệp nhà nước trong kinh tế thành phố, cũng như tách biệt giữa vai trò nhà nước và chức năng thị trường, được đề xuất. Đồng thời, cần tạo điều kiện để thu hút nhân tài, khuyến khích đổi mới và đầu tư công nghệ. Việc thực hiện các giải pháp này cần sự hỗ trợ và quan tâm từ các chính sách và tổ chức chính trị.\n\nPredicted Sentiment: Neutral (Confidence: 0.90)\nProbabilities: {'Negative': 0.010248071514070034, 'Neutral': 0.901475191116333, 'Positive': 0.08827677369117737}\nActual Sentiment: Neutral\n\n=== Sample 4 ===\n\nText: Cá thu giàu dinh dưỡng, đặc biệt vitamin B12, selen và omega-3, cao gấp đôi cá hồi. Ăn cá thu giúp hạ đường huyết, bổ máu, chống ung thư nhờ Coenzyme Q10 và omega-3. Cá thu dễ tiêu hóa, tốt cho gan, thận, tim mạch do cân bằng cholesterol, giảm nguy cơ bệnh tim. Omega-3 trong cá thu cải thiện trí não, giảm nguy cơ bệnh tâm thần, vitamin B12 tăng cường miễn dịch. Vitamin D, đồng, selen trong cá thu giúp xương chắc khỏe.\n\n\nPredicted Sentiment: Positive (Confidence: 0.98)\nProbabilities: {'Negative': 0.0073427604511380196, 'Neutral': 0.017304109409451485, 'Positive': 0.9753531813621521}\nActual Sentiment: Positive\n\n=== Sample 5 ===\n\nText: Sacombank dự kiến mua công ty chứng khoán, đầu tư tối đa 1.500 tỷ đồng và sở hữu trên 50%. Ngân hàng cũng trình phương án tăng vốn điều lệ bằng phát hành cổ phiếu để trả cổ tức và thưởng cho nhân viên từ lợi nhuận giữ lại. Nếu được duyệt, đây là lần chia cổ tức đầu tiên sau 10 năm. Lợi nhuận chưa phân phối năm 2024 là 28.426 tỷ đồng, nợ xấu tăng lên 12.955 tỷ đồng. Sacombank đã nộp hơn 196 tỷ đồng tiền thuế và nghĩa vụ bổ sung cho giai đoạn 2019-2021.\n\n\nPredicted Sentiment: Positive (Confidence: 0.79)\nProbabilities: {'Negative': 0.006877118721604347, 'Neutral': 0.2027684450149536, 'Positive': 0.7903544306755066}\nActual Sentiment: Positive\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Create zip file of all outputs\ndef zip_output_folder(output_dir):\n    zip_path = os.path.join(output_dir, 'output.zip')\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(output_dir):\n            for file in files:\n                if file != 'output.zip':\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, output_dir)\n                    zipf.write(file_path, arcname)\n    return zip_path\n\noutput_zip = zip_output_folder(config.OUTPUT_DIR)\nlogger.info(f\"Created zip file at: {output_zip}\")\n\nprint(\"Training complete! Download the results:\")\nfrom IPython.display import FileLink\nFileLink(output_zip)\n\nlogger.info(\"Training process completed successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:27:26.836767Z","iopub.execute_input":"2025-07-24T13:27:26.837071Z","iopub.status.idle":"2025-07-24T13:36:24.570407Z","shell.execute_reply.started":"2025-07-24T13:27:26.837053Z","shell.execute_reply":"2025-07-24T13:36:24.569680Z"}},"outputs":[{"name":"stdout","text":"Training complete! Download the results:\n","output_type":"stream"}],"execution_count":14}]}