{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:24.148616Z",
     "iopub.status.busy": "2025-07-24T11:57:24.148190Z",
     "iopub.status.idle": "2025-07-24T11:57:27.238612Z",
     "shell.execute_reply": "2025-07-24T11:57:27.237666Z",
     "shell.execute_reply.started": "2025-07-24T11:57:24.148582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:27.240508Z",
     "iopub.status.busy": "2025-07-24T11:57:27.240267Z",
     "iopub.status.idle": "2025-07-24T11:57:30.546934Z",
     "shell.execute_reply": "2025-07-24T11:57:30.545861Z",
     "shell.execute_reply.started": "2025-07-24T11:57:27.240485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.548511Z",
     "iopub.status.busy": "2025-07-24T11:57:30.548246Z",
     "iopub.status.idle": "2025-07-24T11:57:30.554500Z",
     "shell.execute_reply": "2025-07-24T11:57:30.553859Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.548486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import th∆∞ vi·ªán\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    #AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset as HFDataset\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import wandb\n",
    "from rouge_score import rouge_scorer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.555745Z",
     "iopub.status.busy": "2025-07-24T11:57:30.555479Z",
     "iopub.status.idle": "2025-07-24T11:57:30.574648Z",
     "shell.execute_reply": "2025-07-24T11:57:30.574105Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.555722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.576506Z",
     "iopub.status.busy": "2025-07-24T11:57:30.576314Z",
     "iopub.status.idle": "2025-07-24T11:57:30.592903Z",
     "shell.execute_reply": "2025-07-24T11:57:30.592344Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.576491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.593776Z",
     "iopub.status.busy": "2025-07-24T11:57:30.593503Z",
     "iopub.status.idle": "2025-07-24T11:57:30.612241Z",
     "shell.execute_reply": "2025-07-24T11:57:30.611690Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.593755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_config():\n",
    "    \"\"\"T·∫≠p trung to√†n b·ªô c·∫•u h√¨nh v√†o m·ªôt n∆°i.\"\"\"\n",
    "    config = {\n",
    "        \"task_type\": \"summarization\",\n",
    "        \"csv_file_path\": \"/kaggle/input/all-data/data_fireant_0407_final.xlsx - Sheet1.tsv\",\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"model_name\": \"VietAI/vit5-base\",\n",
    "        \"output_dir\": \"./vit5_summarization_finetuned\",\n",
    "        \"max_input_length\": 1024,\n",
    "        \"max_target_length\": 256,\n",
    "        \"validation_split_size\": 0.15,\n",
    "        \"random_state\": 42,\n",
    "        # New parameters to reduce repetition\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"num_beams\": 4,\n",
    "        \"early_stopping\": True,\n",
    "        \"do_sample\": False,  # Set to True for more diverse outputs\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95\n",
    "    }\n",
    "    logger.info(f\"üìä C·∫•u h√¨nh ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p: {config}\")\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.613111Z",
     "iopub.status.busy": "2025-07-24T11:57:30.612939Z",
     "iopub.status.idle": "2025-07-24T11:57:30.636440Z",
     "shell.execute_reply": "2025-07-24T11:57:30.635672Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.613098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"T·∫£i pre-trained model v√† tokenizer.\"\"\"\n",
    "    logger.info(f\"üîÑ ƒêang t·∫£i model v√† tokenizer: '{model_name}'...\")\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    logger.info(f\"‚úÖ T·∫£i th√†nh c√¥ng model '{model_name}' tr√™n device '{device}'\")\n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.637483Z",
     "iopub.status.busy": "2025-07-24T11:57:30.637273Z",
     "iopub.status.idle": "2025-07-24T11:57:30.655677Z",
     "shell.execute_reply": "2025-07-24T11:57:30.655053Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.637464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_tsv(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    T·∫£i d·ªØ li·ªáu t·ª´ file TSV v√† chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng c·∫ßn thi·∫øt.\n",
    "\n",
    "    Args:\n",
    "        file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file TSV.\n",
    "\n",
    "    Returns:\n",
    "        M·ªôt list c√°c dictionary, m·ªói dict ch·ª©a 'input' v√† 'target'.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Reading data from file: {file_path}\")\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "        # Check for required columns\n",
    "        if 'content' not in df.columns or 'summary' not in df.columns:\n",
    "            raise ValueError(\"Error: TSV file must contain 'content' and 'summary' columns.\")\n",
    "\n",
    "        # Rename columns to match format {'input': ..., 'target': ...}\n",
    "        df = df.rename(columns={'content': 'input', 'summary': 'target'})\n",
    "        \n",
    "        # Remove rows with empty values in input or target columns\n",
    "        df.dropna(subset=['input', 'target'], inplace=True)\n",
    "        \n",
    "        # Clean the data - remove extra whitespace and ensure proper format\n",
    "        df['input'] = df['input'].astype(str).str.strip()\n",
    "        df['target'] = df['target'].astype(str).str.strip()\n",
    "        \n",
    "        # Filter out very short summaries that might cause repetition issues\n",
    "        df = df[df['target'].str.len() > 10]\n",
    "        df = df[df['input'].str.len() > 50]\n",
    "\n",
    "        logger.info(f\"‚úÖ Successfully read {len(df)} samples from TSV file.\")\n",
    "        return df.to_dict('records')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"‚ùå File not found at path: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error reading TSV file: {e}\")\n",
    "        raise\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.656746Z",
     "iopub.status.busy": "2025-07-24T11:57:30.656541Z",
     "iopub.status.idle": "2025-07-24T11:57:30.679343Z",
     "shell.execute_reply": "2025-07-24T11:57:30.678836Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.656723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(config: dict, tokenizer: T5Tokenizer):\n",
    "    \"\"\"Load, split and preprocess tokenize data.\"\"\"\n",
    "    logger.info(\"üõ†Ô∏è  Starting data preparation...\")\n",
    "\n",
    "    # Load and split data\n",
    "    all_data = load_data_from_tsv(config['csv_file_path'])\n",
    "    train_data_list, eval_data_list = train_test_split(\n",
    "        all_data,\n",
    "        test_size=config['validation_split_size'],\n",
    "        random_state=config['random_state']\n",
    "    )\n",
    "    train_dataset = HFDataset.from_pandas(pd.DataFrame(train_data_list))\n",
    "    eval_dataset = HFDataset.from_pandas(pd.DataFrame(eval_data_list))\n",
    "    logger.info(f\"‚úÖ Data split: {len(train_dataset)} training samples, {len(eval_dataset)} validation samples.\")\n",
    "\n",
    "    # Preprocessing function\n",
    "    task_prefix = \"summarize: \"\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [task_prefix + doc for doc in examples[\"input\"]]\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, \n",
    "            max_length=config['max_input_length'], \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        # Process targets with proper handling\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"target\"], \n",
    "                max_length=config['max_target_length'], \n",
    "                truncation=True, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        # Replace padding token id with -100 for loss calculation\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_train_dataset = train_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    tokenized_eval_dataset = eval_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    logger.info(\"‚úÖ Data tokenized successfully.\")\n",
    "    \n",
    "    return tokenized_train_dataset, tokenized_eval_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.680209Z",
     "iopub.status.busy": "2025-07-24T11:57:30.680017Z",
     "iopub.status.idle": "2025-07-24T11:57:30.700222Z",
     "shell.execute_reply": "2025-07-24T11:57:30.699709Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.680186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"VietAI/vit5-base\", legacy=False)\n",
    "    \n",
    "    # Replace -100 with pad token id for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(label, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge1_scores),\n",
    "        'rouge2': np.mean(rouge2_scores),\n",
    "        'rougeL': np.mean(rougeL_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.701049Z",
     "iopub.status.busy": "2025-07-24T11:57:30.700878Z",
     "iopub.status.idle": "2025-07-24T11:57:30.724636Z",
     "shell.execute_reply": "2025-07-24T11:57:30.724058Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.701035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_trainer(model, tokenizer, config, train_dataset, eval_dataset):\n",
    "    \"\"\"Configure and initialize Hugging Face Trainer with improved settings.\"\"\"\n",
    "    logger.info(\"‚öôÔ∏è  Configuring Hugging Face Trainer...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        num_train_epochs=config['num_epochs'],\n",
    "        per_device_train_batch_size=config['batch_size'],\n",
    "        per_device_eval_batch_size=config['batch_size'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{config['output_dir']}/logs\",\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        greater_is_better=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        # Additional parameters to improve training stability\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, \n",
    "        model=model, \n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=None,  # We'll compute metrics separately\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.725486Z",
     "iopub.status.busy": "2025-07-24T11:57:30.725328Z",
     "iopub.status.idle": "2025-07-24T11:57:30.746874Z",
     "shell.execute_reply": "2025-07-24T11:57:30.746262Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.725473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_summary_with_improved_settings(model, tokenizer, input_text, config, device):\n",
    "    \"\"\"Generate summary with improved settings to reduce repetition.\"\"\"\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = \"summarize: \" + input_text\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=config['max_input_length'],\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # Generate with improved parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=config['max_target_length'],\n",
    "            min_length=30,  # Ensure minimum length\n",
    "            num_beams=config['num_beams'],\n",
    "            repetition_penalty=config['repetition_penalty'],\n",
    "            no_repeat_ngram_size=config['no_repeat_ngram_size'],\n",
    "            length_penalty=config['length_penalty'],\n",
    "            early_stopping=config['early_stopping'],\n",
    "            do_sample=config['do_sample'],\n",
    "            temperature=config['temperature'] if config['do_sample'] else None,\n",
    "            top_k=config['top_k'] if config['do_sample'] else None,\n",
    "            top_p=config['top_p'] if config['do_sample'] else None,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated summary\n",
    "    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T11:57:30.748051Z",
     "iopub.status.busy": "2025-07-24T11:57:30.747799Z",
     "iopub.status.idle": "2025-07-24T13:10:18.330392Z",
     "shell.execute_reply": "2025-07-24T13:10:18.329546Z",
     "shell.execute_reply.started": "2025-07-24T11:57:30.748036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfc335e1be943bfbc883ba371a0e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b28efc150ef413098a5f133f11159a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/827 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3513' max='3513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3513/3513 1:10:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.234600</td>\n",
       "      <td>1.127015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.032000</td>\n",
       "      <td>1.097187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>1.102287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "üìå Input: ƒê·ªãa ·ªëc First Real d·ª± ki·∫øn ph√°t h√†nh h∆°n 6,4 tri·ªáu c·ªï phi·∫øu th∆∞·ªüng cho c·ªï ƒë√¥ng nh·∫±m tƒÉng v·ªën ƒëi·ªÅu l·ªá, ng√†y ƒëƒÉng k√Ω cu·ªëi c√πng ƒë·ªÉ ph√¢n b·ªï quy·ªÅn l√† 30/7/2025.\n",
      "C√¥ng ty C·ªï ph·∫ßn ƒê·ªãa ·ªëc First Real (MCK: FIR, ...\n",
      "üí° Improved Output: ƒê·ªãa ·ªëc First Real d·ª± ki·∫øn ph√°t h√†nh h∆°n 6,4 tri·ªáu c·ªï phi·∫øu th∆∞·ªüng cho c·ªï ƒë√¥ng ƒë·ªÉ tƒÉng v·ªën ƒëi·ªÅu l·ªá, ng√†y ƒëƒÉng k√Ω cu·ªëi c√πng l√† 30/7/2025. T·ªïng gi√° tr·ªã ph√°t h√†nh l√† h∆°n 64,2 t·ª∑ ƒë·ªìng, ƒë∆∞·ª£c l·∫•y t·ª´ th·∫∑ng d∆∞ v·ªën c·ªï ph·∫ßn c·ªßa c√¥ng ty. Ph∆∞∆°ng √°n n√†y ƒë√£ ƒë∆∞·ª£c c·ªï ƒë√¥ng th√¥ng qua t·∫°i ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng th∆∞·ªùng ni√™n nƒÉm 2015. D·ª± ki·∫øn s·ªë l∆∞·ª£ng c·ªï phi·∫øu ƒë√£ ph√°t h√†nh s·∫Ω tƒÉng t·ª´ g·∫ßn 642,5 t·ª∑ ƒë·ªìng l√™n g·∫ßn 706,7 t·ª∑ ƒë·ªìng.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def run_training_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire fine-tuning process.\n",
    "    \"\"\"\n",
    "    logger.info(\"üöÄ STARTING VIT5 FINE-TUNING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    config = setup_config()\n",
    "    model, tokenizer, device = load_model_and_tokenizer(config['model_name'])\n",
    "    train_ds, eval_ds = prepare_datasets(config, tokenizer)\n",
    "    trainer = setup_trainer(model, tokenizer, config, train_ds, eval_ds)\n",
    "    \n",
    "    logger.info(\"üéØ Starting fine-tuning process...\")\n",
    "    trainer.train()\n",
    "    logger.info(\"‚úÖ Fine-tuning completed!\")\n",
    "    \n",
    "    final_model_path = f\"{config['output_dir']}/final_model\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    logger.info(f\"üíæ Best model saved at: {final_model_path}\")\n",
    "\n",
    "    logger.info(\"üìà Evaluating model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    logger.info(f\"   - Eval Loss: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "    logger.info(f\"   - Eval Runtime: {eval_results.get('eval_runtime', 'N/A')}s\")\n",
    "\n",
    "    logger.info(\"\\nüß™ Testing fine-tuned model:\")\n",
    "    test_input = \"\"\"ƒê·ªãa ·ªëc First Real d·ª± ki·∫øn ph√°t h√†nh h∆°n 6,4 tri·ªáu c·ªï phi·∫øu th∆∞·ªüng cho c·ªï ƒë√¥ng nh·∫±m tƒÉng v·ªën ƒëi·ªÅu l·ªá, ng√†y ƒëƒÉng k√Ω cu·ªëi c√πng ƒë·ªÉ ph√¢n b·ªï quy·ªÅn l√† 30/7/2025.\n",
    "C√¥ng ty C·ªï ph·∫ßn ƒê·ªãa ·ªëc First Real (MCK: FIR, s√†n HoSE) v·ª´a c√≥ vƒÉn b·∫£n th√¥ng b√°o v·ªÅ ph√°t h√†nh c·ªï phi·∫øu ƒë·ªÉ tƒÉng v·ªën c·ªï ph·∫ßn t·ª´ ngu·ªìn v·ªën ch·ªß s·ªü h·ªØu.\n",
    "Theo ƒë√≥, ƒê·ªãa ·ªëc First Real d·ª± ki·∫øn ph√°t h√†nh h∆°n 6,4 tri·ªáu c·ªï phi·∫øu cho c·ªï ƒë√¥ng hi·ªán h·ªØu v·ªõi t·ª∑ l·ªá th·ª±c hi·ªán quy·ªÅn 10:1, t·ª©c c·ªï ƒë√¥ng s·ªü h·ªØu 1 c·ªï phi·∫øu ƒë∆∞·ª£c h∆∞·ªüng 1 quy·ªÅn, c·ª© 10 quy·ªÅn s·∫Ω ƒë∆∞·ª£c nh·∫≠n 1 c·ªï phi·∫øu m·ªõi. Ng√†y ƒëƒÉng k√Ω cu·ªëi c√πng ƒë·ªÉ ph√¢n b·ªï quy·ªÅn l√† 30/7/2025.\n",
    "T·ªïng gi√° tr·ªã ph√°t h√†nh t√≠nh theo m·ªánh gi√° l√† h∆°n 64,2 t·ª∑ ƒë·ªìng. Ngu·ªìn v·ªën th·ª±c hi·ªán ƒë∆∞·ª£c l·∫•y t·ª´ ngu·ªìn th·∫∑ng d∆∞ v·ªën c·ªï ph·∫ßn c·ªßa c√¥ng ty theo b√°o c√°o t√†i ch√≠nh nƒÉm 2024 ƒë√£ ki·ªÉm to√°n.\n",
    "·∫¢nh minh h·ªça\n",
    "N·∫øu ƒë·ª£t ph√°t h√†nh th√†nh c√¥ng, s·ªë l∆∞·ª£ng c·ªï phi·∫øu ƒë√£ ph√°t h√†nh c·ªßa ƒê·ªãa ·ªëc First Real s·∫Ω tƒÉng t·ª´ h∆°n 64,2 tri·ªáu c·ªï phi·∫øu l√™n g·∫ßn 70,7 tri·ªáu c·ªï phi·∫øu, t∆∞∆°ng ƒë∆∞∆°ng v·ªën ƒëi·ªÅu l·ªá tƒÉng t·ª´ g·∫ßn 642,5 t·ª∑ ƒë·ªìng l√™n g·∫ßn 706,7 t·ª∑ ƒë·ªìng.\n",
    "ƒê∆∞·ª£c bi·∫øt, ph∆∞∆°ng √°n ph√°t h√†nh c·ªï phi·∫øu n√†y ƒë√£ ƒë∆∞·ª£c c·ªï ƒë√¥ng c·ªßa ƒê·ªãa ·ªëc First Real th√¥ng qua t·∫°i ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng (ƒêHƒêCƒê) th∆∞·ªùng ni√™n 2025 ƒë∆∞·ª£c t·ªï ch·ª©c ng√†y 21/3/2025.\"\"\"\n",
    "        \n",
    "    loaded_tokenizer = T5Tokenizer.from_pretrained(final_model_path, legacy=False)\n",
    "    loaded_model = T5ForConditionalGeneration.from_pretrained(final_model_path).to(device)\n",
    "\n",
    "    # Test with improved generation settings\n",
    "    prediction = generate_summary_with_improved_settings(\n",
    "        loaded_model, loaded_tokenizer, test_input, config, device\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"üìå Input: {test_input[:200]}...\")\n",
    "    print(f\"üí° Improved Output: {prediction}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_training_pipeline()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Critical error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r vit5_summarization_finetuned.zip /kaggle/working/vit5_summarization_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7793069,
     "sourceId": 12360639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7801530,
     "sourceId": 12373023,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7807248,
     "sourceId": 12381576,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7883745,
     "sourceId": 12492789,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7884099,
     "sourceId": 12493276,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
