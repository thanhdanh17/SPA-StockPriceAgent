# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import pandas as pd


# def crawl_detail_article_selenium(driver, article_url):
#     """D√πng Selenium ƒë·ªÉ m·ªü b√†i vi·∫øt v√† l·∫•y n·ªôi dung t·ª´ th·∫ª .article__body.cms-body"""
#     try:
#         driver.get(article_url)
#         WebDriverWait(driver, 10).until(
#             EC.presence_of_element_located((By.CLASS_NAME, "article__body"))
#         )
#         soup = BeautifulSoup(driver.page_source, "html.parser")
#         content_div = soup.find("div", class_="article__body")
#         if content_div:
#             return content_div.get_text(separator="\n", strip=True)
#         else:
#             return None
#     except Exception as e:
#         print(f"‚ö†Ô∏è L·ªói khi l·∫•y n·ªôi dung b·∫±ng Selenium: {e}")
#         return None



# def crawl_tag_page(url, max_articles=20, wait_time=10):
#     """L·∫•y danh s√°ch b√†i t·ª´ trang tag v√† m·ªü t·ª´ng b√†i ƒë·ªÉ l·∫•y n·ªôi dung"""
#     options = Options()
#     # B·ªè comment d√≤ng d∆∞·ªõi n·∫øu mu·ªën ch·∫°y kh√¥ng hi·ªán tr√¨nh duy·ªát
#     # options.add_argument("--headless")
#     driver = webdriver.Chrome(options=options)

#     print(f"üöÄ ƒêang m·ªü: {url}")
#     driver.get(url)

#     # Ch·ªù b√†i vi·∫øt xu·∫•t hi·ªán
#     try:
#         WebDriverWait(driver, wait_time).until(
#             EC.presence_of_element_located((By.CLASS_NAME, "story"))
#         )
#     except:
#         print("‚ùå Kh√¥ng th·∫•y b√†i vi·∫øt n√†o.")
#         driver.quit()
#         return []

#     soup = BeautifulSoup(driver.page_source, "html.parser")
#     article_elements = soup.find_all("article", class_="story")

#     print(f"üìå S·ªë b√†i t√¨m ƒë∆∞·ª£c: {len(article_elements)}")

#     results = []
#     for art in article_elements[:max_articles]:
#         try:
#             title = art.find("h2", class_="story__heading").text.strip()
#             url = art.find("a")["href"]
#             date = art.find("div", class_="story__meta").text.strip() if art.find("div", class_="story__meta") else "N/A"

#             print(f"üìñ {title}\n‚Üí {url}")
#             body = crawl_detail_article_selenium(driver, url)
#             print(f"üìù N·ªôi dung: {body[:150]}...\n" if body else "‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung.\n")

#             results.append({
#                 "title": title,
#                 "url": url,
#                 "date": date,
#                 "body": body
#             })
#         except Exception as e:
#             print("‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω b√†i:", e)

#     driver.quit()
#     return results


# def save_to_csv(articles, filename="duoc_articles.csv"):
#     df = pd.DataFrame(articles)
#     df.to_csv(filename, index=False)
#     print(f"‚úÖ ƒê√£ l∆∞u {len(df)} b√†i vi·∫øt v√†o {filename}")




from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time


def click_load_more(driver, max_clicks=30):
    """T·ª± ƒë·ªông b·∫•m n√∫t 'XEM TH√äM' nhi·ªÅu l·∫ßn ƒë·ªÉ load th√™m b√†i"""
    for i in range(max_clicks):
        try:
            btn = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "control__loadmore"))
            )
            driver.execute_script("arguments[0].click();", btn)
            print(f"üîÅ ƒê√£ b·∫•m XEM TH√äM l·∫ßn {i+1}")
            time.sleep(2)
        except:
            print("‚õî Kh√¥ng c√≤n n√∫t XEM TH√äM ho·∫∑c b·ªã ·∫©n.")
            break


def crawl_detail_article_selenium(driver, article_url):
    """D√πng Selenium ƒë·ªÉ m·ªü b√†i vi·∫øt v√† l·∫•y n·ªôi dung t·ª´ th·∫ª .article__body.cms-body"""
    try:
        driver.get(article_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "article__body"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")
        content_div = soup.find("div", class_="article__body")
        if content_div:
            return content_div.get_text(separator="\n", strip=True)
        else:
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi l·∫•y n·ªôi dung b·∫±ng Selenium: {e}")
        return None


def crawl_page(driver, url, tag_name="unknown", max_articles=500, wait_time=10):
    print(f"\nüöÄ ƒêang crawl: {url}")
    driver.get(url)

    # B·∫•m "XEM TH√äM" nhi·ªÅu l·∫ßn n·∫øu c√≥
    click_load_more(driver, max_clicks=10)

    try:
        WebDriverWait(driver, wait_time).until(
            EC.presence_of_element_located((By.CLASS_NAME, "story"))
        )
    except:
        print("‚ùå Kh√¥ng th·∫•y b√†i vi·∫øt n√†o.")
        return []

    soup = BeautifulSoup(driver.page_source, "html.parser")
    article_elements = soup.find_all("article", class_="story")
    print(f"üìå S·ªë b√†i tr√™n trang sau load th√™m: {len(article_elements)}")

    results = []
    for art in article_elements[:max_articles]:
        try:
            title = art.find("h2", class_="story__heading").text.strip()
            url = art.find("a")["href"]
            date = art.find("div", class_="story__meta").text.strip() if art.find("div", class_="story__meta") else "N/A"

            print(f"üìñ {title}\n‚Üí {url}")
            body = crawl_detail_article_selenium(driver, url)
            print(f"üìù N·ªôi dung: {body[:150]}...\n" if body else "‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung.\n")

            results.append({
                "title": title,
                "link": url,
                "published": date,
                "content": body
            })
        except Exception as e:
            print("‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω b√†i:", e)

    return results


def crawl_multiple_pages(pages: dict, max_articles=500):
    """pages: dict { 'ten_tag': 'link' }"""
    options = Options()
    # options.add_argument("--headless")  # B·∫≠t n·∫øu mu·ªën ch·∫°y ng·∫ßm
    driver = webdriver.Chrome(options=options)

    all_results = []
    for tag, url in pages.items():
        articles = crawl_page(driver, url, tag_name=tag, max_articles=max_articles)
        all_results.extend(articles)

    driver.quit()
    return all_results


def save_to_csv(articles, filename="energy_01.csv"):
    df = pd.DataFrame(articles)
    df.to_csv(filename, index=False)
    print(f"‚úÖ ƒê√£ l∆∞u {len(df)} b√†i vi·∫øt v√†o {filename}")
